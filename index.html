<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="ali.css" type="text/css" />
<style>
#paper{color: #A52A2A}
#paperlink{color: #C0C0C0}
#papercode{color: #3CB371}
#paperdata{color: #FFA500}
#paperslides{color: #FF69B4}
#papercomments{width: 700px; font-family: Helvetica}
#paperabstract{width: 700px; font-size: 1}
#papertitle{width: 700px; font-size:3}
</style>
<title>Jian Liang</title>
<link rel="shortcut icon" href="assets/JL.ico" />
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>

<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="experience.html" class="current">Experience</a></div>
<div class="menu-item"><a href="education.html">Education</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
<div class="menu-item"><a href="activity.html">Activity</a></div>
<div class="menu-item"><a href="miscellaneous.html">Miscellaneous</a></div>
</td>
<td id="layout-content">
<h1>About Me</h1>
<p><div><img src="assets/liangjian@sigir.jpg" alt="Jian Liang" style="float:left" hspace="10" width="200" height="280"></div> 
</p>
<p>
<font size="4"><b>梁坚 (Jian Liang, Tim)</b></font><br />
Associate Professor<br />
Center for Research on Intelligent Perception and Computing<br />
Institute of Automation, Chinese Academy of Sciences<br /><br />
Room 1505, Intelligent Building, 95 Zhongguancun East Road<br />
100190, Haidian District, Beijing, China<br><br>
<font size='2'>&#x1F47B</font><a href="assets/liangjian-cv.pdf" target=&ldquo;blank&rdquo;>Resume</a><br />
<font size='4'>&#x265F</font><a href="https://github.com/tim-learn" target=&ldquo;blank&rdquo;>Github</a><br />
<font size='4'>&#x1F396</font><a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a><br />
<font size='2'>&#x1F4C4</font><a href="https://arxiv.org/a/liang_j_1.html" target=&ldquo;blank&rdquo;>Arxiv</a><br />
&nbsp;<font size='2'>&#x21F0</font> <a href="mailto:liangjian92@gmail.com" target=&ldquo;blank&rdquo;>liangjian92&#x1F300gmail.com</a> or <a href="mailto:jian.liang@nlpr.ia.ac.cn" target=&ldquo;blank&rdquo;>jian.liang&#x1F300nlpr.ia.ac.cn</a>
</p>
<br />

<p>Before joining CASIA in June 2021, I was a research fellow at the Vision and Learning Group, National University of Singapore, working with <a href="https://sites.google.com/site/jshfeng/">Dr. Jiashi Feng</a> from June 2019 to April 2021.
I obtained Ph.D. in <i>Pattern Recognition and Intelligent Systems</i> from CASIA in Jan 2019, under the supervision of <a href="http://cripac.ia.ac.cn/en/EN/column/item80.shtml">Prof. Tieniu Tan</a> and co-supervision of <a href="http://people.ucas.ac.cn/~heran">Prof. Ran He</a> and <a href="http://cripac.ia.ac.cn/en/EN/column/item110.shtml">Prof. Zhenan Sun</a>, and received my bachelor degree in <i>Automation</i> from Xi'an Jiaotong University in June 2013.</p>
<p>My current research interests mainly focus on representation learning, knowledge transfer, trustworthy AI (including security, privacy, or robustness in AI), and their applications in various computer vision problems.</p>
<!-- <ul>
<li>domain adaptation, model adaptation, test-time adaptation</li>
<li>non-iid federated learning, personalized federated learning</li>
<li>model attacks</li>
<li>open-world learning</li>
<li>face-related applications, deepfake detection</li>
</ul> -->

<div class="infoblock">
<div class="blockcontent">
<p><p style="text-align:center;"><i>
I am looking for a master student who will start in 2024.
<br>
I am open to discussion or collaboration. Feel free to drop me an email if you're interested.
</i></p></p>
</div></div>

<h1>News</h1>
<ul>
<li><p>2023/07/14 Four papers have been accepted to ICCV 2023.</p></li>
<li><p>2023/07/04 <a href="https://github.com/yuyongcan/Benchmark-TTA">Benchmark-TTA</a> (a benchmark for test-time adaptation) has been released.</p></li>
<li><p>2023/07/02 <a href="https://github.com/YuheD/MAPS">MAPS</a> has been accepted to IEEE TCSVT.</p></li>
</ul>


<h1>Research Group</h1>
<ul>
<li><p>Jiyang Guan (with Prof. He), PhD @ CASIA</p></li>
<li><p><a href="https://tomsheng21.github.io/">Lijun Sheng</a> (with Prof. Tan), PhD @ USTC</p></li>
<li><p>Yuhe Ding (with Prof. Jiang), PhD @ AHU</p></li>
<li><p>Aijing Yu (with Prof. Zhang), PhD @ IIE, CAS</p></li>
<li><p>Yuting Xu (with Prof. Zhang), PhD @ IIE, CAS</p></li>
<li><p>Puning Yang (with Prof. He), PhD @ CASIA</p></li>
<li><p>Zhengbo Wang (with Prof. Tan), PhD @ USTC</p></li>
<li><p>Yanbo Wang (with Prof. He), PhD @ CASIA</p></li>
<li><p>Yongcan Yu, Master @ CASIA</p></li>
<li><p>Kuangpo Guo (with Prof. Tan), Master @ USTC</p></li>
<!-- <li><p>Xiaokun Yang (with Prof. Tan), PhD @ CASIA</p></li>(incoming) -->
</ul>


<h1>Research Highlights</h1>
<details><summary>
<span id="papertitle">A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts.
</span></a><br></summary>
</details>

<b>Jian Liang</b>, Ran He, Tieniu Tan.<br>
<i>under review, 2023.</i><br>
[<a href="https://arxiv.org/pdf/2303.15361.pdf"><span id="paper">Paper</span></a>]
[<a href="https://github.com/tim-learn/awesome-test-time-adaptation"><span id="papercode">Code</span></a>]
<br><br>


<details><summary>
<span id="papertitle">Benchmarking Test-Time Adaptation against Distribution Shifts in Image Classification.
</span></a><br></summary>
</details>

Yongcan Yu, Lijun Sheng, Ran He, <b>Jian Liang</b>.<br>
<i>Arxiv technical report, 2023.</i><br>
[<a href="https://arxiv.org/pdf/2307.03133.pdf"><span id="paper">Paper</span></a>]
[<a href="https://github.com/yuyongcan/Benchmark-TTA"><span id="papercode">Code</span></a>]
<br><br>


<!-- 
<h1>Recent Publications (<a href="publications.html">Full list</a>)</h1>
<details><summary>
<span id="papertitle">A Comprehensive Survey on Test-Time Adaptation under Distribution Shifts.
</span></a><br></summary>
<div id="abstract"><i>
Abstract: 
</i></div></details>

<b>Jian Liang</b>, Ran He, Tieniu Tan.<br>
<i>under review, 2023.</i><br>
[<a href="https://arxiv.org/pdf/2303.15361.pdf"><span id="paper">Paper</span></a>]
[<a href="https://github.com/tim-learn/awesome-test-time-adaptation"><span id="papercode">Code</span></a>]
<br><br>


<details><summary>
<span id="papertitle">DINE: Domain Adaptation from Single and Multiple Black-box Predictors</span></a><br>
</summary>
<div id="paperabstract"><i>
<dd><br>
Abstract: To ease the burden of labeling, unsupervised domain adaptation (UDA) aims to transfer knowledge in previous and related labeled datasets (sources) to a new unlabeled dataset (target). Despite impressive progress, prior methods always need to access the raw source data and develop data-dependent alignment approaches to recognize the target samples in a transductive learning manner, which may raise privacy concerns from source individuals. Several recent studies resort to an alternative solution by exploiting the well-trained white-box model from the source domain, yet, it may still leak the raw data through generative adversarial learning. This paper studies a practical and interesting setting for UDA, where only black-box source models (i.e., only network predictions are available) are provided during adaptation in the target domain. To solve this problem, we propose a new two-step knowledge adaptation framework called DIstill and fine-tuNE (DINE). Taking into consideration the target data structure, DINE first distills the knowledge from the source predictor to a customized target model, then fine-tunes the distilled model to further fit the target domain. Besides, neural networks are not required to be identical across domains in DINE, even allowing effective adaptation on a low-resource device. Empirical results on three UDA scenarios (i.e., single-source, multi-source, and partial-set) confirm that DINE achieves highly competitive performance compared to state-of-the-art data-dependent approaches.
</dd><br>
</i></div>
</details>

<b>Jian Liang</b>, Dapeng Hu, Jiashi Feng, Ran He.<br>
<i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022 (Oral)</i><br>
[<a href="https://arxiv.org/pdf/2104.01539.pdf"><span id="paper">Paper</span></a>]
[<a href="https://github.com/tim-learn/DINE"><span id="papercode">Code</span></a>]
[<a href="./assets/talk/cvpr22a_slides.pdf"><span id="paperslides">Slides</span></a>]
<br>
<br>
<div id="comments">This paper studies a practical and interesting setting for UDA, where only black-box source models (i.e., only network predictions are available) are provided during adaptation in the target domain.</div>
<br>
<br>

<details><summary>
<span id="papertitle">Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class Incremental Learning</span></a><br>
</summary>
<div id="paperabstract"><i>
Abstract: Class Incremental Learning (CIL) aims at learning a multi-class classifier in a phase-by-phase manner, in which only data of a subset of the classes are provided at each phase. Previous works mainly focus on mitigating forgetting in phases after the initial one. However, we find that improving CIL at its initial phase is also a promising direction. Specifically, we experimentally show that directly encouraging CIL Learner at the initial phase to output similar representations as the model jointly trained on all classes can greatly boost the CIL performance. Motivated by this, we study the difference between a naïvely-trained initial-phase model and the oracle model. Specifically, since one major difference between these two models is the number of training classes, we investigate how such difference affects the model representations. We find that, with fewer training classes, the data representations of each class lie in a long and narrow region; with more training classes, the representations of each class scatter more uniformly. Inspired by this observation, we propose Class-wise Decorrelation (CwD) that effectively regularizes representations of each class to scatter more uniformly, thus mimicking the model jointly trained with all classes (i.e., the oracle model). Our CwD is simple to implement and easy to plug into existing methods. Extensive experiments on various benchmark datasets show that CwD consistently and significantly improves the performance of existing state-of-the-art methods by around 1\% to 3\%.
</i></div>
</details>

Yujun Shi, Kuangqi Zhou, <b>Jian Liang</b>, Zihang Jiang, Jiashi Feng, Philip Torr, Song Bai, Vincent Y. F. Tan.<br>
<i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022</i><br>
[<a href="https://arxiv.org/pdf/2112.04731.pdf"><span id="paper">Paper</span></a>]
[<a href="https://github.com/Yujun-Shi/CwD"><span id="papercode">Code</span></a>]
<br>
<br>
<div id="comments">We propose a Class-wise Decorrelation regularizer that enables CIL learner at initial phase to mimic representations produced by the oracle model (the model jointly trained on all classes) and thus boosting Class Incremental Learning.</div>
<br>
<br>


<details><summary>
<span id="papertitle">META: Mimicking Embedding via oThers' Aggregation for Generalizable Person Re-identification.
</span></a><br></summary>
<div id="paperabstract"><i>
Abstract: Domain generalizable (DG) person re-identification (ReID) aims to test across unseen domains without access to the target domain data at training time, which is a realistic but challenging problem. 
In contrast to methods assuming an identical model for different domains, Mixture of Experts (MoE) exploits multiple domain-specific networks for leveraging complementary information between domains, obtaining impressive results. 
However, prior MoE-based DG ReID methods suffer from a large model size with the increase of the number of source domains, and most of them overlook the exploitation of domain-invariant characteristics. To handle the two issues above, this paper presents a new approach called Mimic Embedding via adapTive Aggregation ($\mathsf{META}$) for DG person ReID. 
To avoid the large model size, experts in $\mathsf{META}$ do not adopt a branch network for each source domain but share all the parameters except for the batch normalization layers.
Besides multiple experts, $\mathsf{META}$ leverages Instance Normalization (IN) and introduces it into a global branch to pursue invariant features across domains.
Meanwhile, $\mathsf{META}$ considers the relevance of an unseen target sample and source domains via normalization statistics and develops an aggregation module to adaptively integrate multiple experts for mimicking unseen target domain. 
Benefiting from a proposed consistency loss and an episodic training algorithm, $\mathsf{META}$ is expected to mimic embedding for a truly unseen target domain. 
Extensive experiments verify that $\mathsf{META}$ surpasses state-of-the-art DG person ReID methods by a large margin.
</i></div></details>

Boqiang Xu, <b>Jian Liang</b>, Lingxiao He, Zhenan Sun.<br>
<i>European Conference on Computer Vision (ECCV), 2022.</i><br>
[<a href="https://arxiv.org/pdf/2112.08684.pdf"><span id="paper">Paper</span></a>]
[<a href="https://github.com/xbq1994/meta"><span id="papercode">Code</span></a>]
<br><br>

<div id="comments">We propose a new domain generalization method for person re-identification.</div>
<br>
<br>


<details><summary>
<span id="papertitle">Are You Stealing My Model? Sample Correlation for Fingerprinting Deep Neural Networks.
</span></a><br></summary>
<div id="paperabstract"><i>
Abstract: An off-the-shelf model as a commercial service could be stolen by model stealing attacks, posing great threats to the rights of the model owner. Model fingerprinting aims to verify whether a suspect model is stolen from the victim model, which gains more and more attention nowadays. Previous methods always leverage the transferable adversarial examples as the model fingerprint, which is sensitive to adversarial defense or transfer learning scenarios. To address this issue, we consider the pairwise relationship between samples instead and propose a novel yet simple model stealing detection method based on SAmple Correlation (SAC). Specifically, we present SAC-w that selects wrongly classified normal samples as model inputs and calculates the mean correlation among their model outputs. To reduce the training time, we further develop SAC-m that selects CutMix Augmented samples as model inputs, without the need for training the surrogate models or generating adversarial examples. Extensive results validate that SAC successfully defends against various model stealing attacks, even including adversarial training or transfer learning, and detects the stolen models with the best performance in terms of AUC across different datasets and model architectures.
</i></div></details>

Jiyang Guan, <b>Jian Liang</b>, Ran He.<br>
<i>Annual Conference on Neural Information Processing Systems (NeurIPS), 2022</i><br>
[<a href="./assets/paper/neurips22.pdf"><span id="paper">Paper</span></a>]
[<a href="https://github.com/guanjiyang/SAC"><span id="papercode">Code</span></a>]
<br><br>

<div id="comments">We propose a simple yet effective method for fingerprinting deep models.</div>
<br>
<br> -->

<br>
<div id="footer">
<div id="footer-text">
Last updated on July 17, 2023. The CSS file is adapted from <a href="https://people.eecs.berkeley.edu/~brecht/" target="blank">Ben Recht's webpage</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
