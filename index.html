<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="ali.css" type="text/css" />
<style>
body{font-family: Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif; font-size: 16px;}
#paper{color: #A52A2A}
#paperlink{color: #C0C0C0}
#papercode{color: #3CB371}
#paperdata{color: #FFA500}
#paperslides{color: #FF69B4}
#papercomments{width: 700px; font-family: Helvetica}
#paperabstract{width: 700px; font-size: 1}
#papertitle{width: 700px; font-size:3}
#focus{color: #B010A5; font-weight: bold}
</style>
<title>Jian Liang</title>
<link rel="shortcut icon" href="assets/JL.ico" />
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>

<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="experience.html" class="current">Experience</a></div>
<div class="menu-item"><a href="education.html">Education</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
<div class="menu-item"><a href="activity.html">Activities</a></div>
<div class="menu-item"><a href="miscellaneous.html">Miscellaneous</a></div>
</td>
<td id="layout-content">
<h1>About Me</h1>
<p><div><img src="assets/liangjian@iccv.jpg" alt="Jian Liang" style="float:left" hspace="10" width="200" height="250"></div> 
</p>
<p>
<font size="4"><b>梁坚 (Jian Liang, Tim)</b></font><br>
Associate Professor<br><br>
NLPR, Institute of Automation, Chinese Academy of Sciences<br>
95 Zhongguancun East Road, 100190, Haidian, Beijing<br><br>
<font size='2'>&#x1F47B</font><a href="assets/liangjian-cv.pdf" target=&ldquo;blank&rdquo;>Resume</a><br>
<font size='4'>&#x265F</font><a href="https://github.com/tim-learn" target=&ldquo;blank&rdquo;>Github</a><br>
<font size='4'>&#x1F396</font><a href="https://scholar.google.com/citations?user=eY8i-mQAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a><br>
<font size='2'>&#x1F4C4</font><a href="https://arxiv.org/a/liang_j_1.html" target=&ldquo;blank&rdquo;>Arxiv</a><br>
&nbsp;<font size='2'>&#x21F0</font> <a href="mailto:liangjian92@gmail.com" target=&ldquo;blank&rdquo;>liangjian92&#x1F300gmail.com</a> or <a href="mailto:jian.liang@nlpr.ia.ac.cn" target=&ldquo;blank&rdquo;>jian.liang&#x1F300nlpr.ia.ac.cn</a>
</p>
<br>

<p>Before joining CASIA in June 2021, I was a research fellow at the Vision and Learning Group, National University of Singapore, working with <a href="https://sites.google.com/site/jshfeng/">Dr. Jiashi Feng</a> from June 2019 to April 2021.
I obtained Ph.D. in <i>Pattern Recognition and Intelligent Systems</i> from CASIA in Jan 2019, under the supervision of <a href="http://cripac.ia.ac.cn/en/EN/column/item80.shtml">Prof. Tieniu Tan</a> and co-supervision of <a href="http://people.ucas.ac.cn/~heran">Prof. Ran He</a> and <a href="http://cripac.ia.ac.cn/en/EN/column/item110.shtml">Prof. Zhenan Sun</a>, and received my bachelor degree in <i>Automation</i> from Xi'an Jiaotong University in June 2013.</p>
<p>My current research interests mainly focus on representation learning, knowledge transfer, trustworthy AI (including security, privacy, or robustness in AI), and their applications in various computer vision problems.</p>
<!-- <ul>
<li>domain adaptation, model adaptation, test-time adaptation</li>
<li>non-iid federated learning, personalized federated learning</li>
<li>model attacks</li>
<li>open-world learning</li>
<li>face-related applications, deepfake detection</li>
</ul> -->

<div class="infoblock">
<div class="blockcontent">
<p><p style="text-align:center;"><i>
I am open to discussion or collaboration. Feel free to drop me an email if you're interested.
</i></p></p>
</div></div>

<h1>News (the past year)</h1>
<ul>
<li><p>[2025/08] I am invited as an Area Chair of ICLR 2026.</p></li>
<li><p>[2025/08] One paper has been accepted to IEEE TMM.</p></li>
<li><p>[2025/08] One paper has been accepted to ACM CSUR.</p></li>
<li><p>[2025/07] One paper has been accepted to ACM-MM 2025.</p></li>
<li><p>[2025/06] One paper has been accepted to ICCV 2025.</p></li>
<li><p>[2025/06] I am invited as an Area Chair of WACV 2026.</p></li>
<li><p>[2025/02] Two papers have been accepted to CVPR 2025.</p></li>
<li><p>[2025/01] One paper has been accepted to ICLR 2025.</p></li>	
<li><p>[2024/12] I am invited as an Area Chair of ICCV 2025.</p></li>
<li><p>[2024/12] I am invited as an Associate Editor of Pattern Recognition.</p></li>
<li><p>[2024/12] Two papers have been accepted to AAAI 2025.</p></li>
<li><p>[2024/12] I am invited as an Area Chair of IJCAI 2025.</p></li>
<li><p>[2024/11] I am invited as an Area Chair of ICML 2025.</p></li>
<li><p>[2024/10] Our extension on model fingerprinting has been accepted to IJCV.</p></li>
<li><p>[2024/09] One paper has finally been accepted to NeurIPS DB Track 2024.</p></li>
<li><p>[2024/08] I am invited as an Area Chair of ICLR 2025.</p></li>
<!-- <li><p>[2024/07] One paper has finally been accepted to TPAMI.</p></li> -->
<!-- <li><p>[2024/07] One paper has been accepted to IJCV.</p></li> -->
<!-- <li><p>[2024/07] One paper has been accepted to ECCV 2024.</p></li> -->
<!-- <li><p>[2024/05] Three papers have been accepted to ICML 2024.</p></li> -->
<!-- <li><p>[2024/05] I am invited as an Area Chair of NeurIPS 2024.</p></li> -->
<!-- <li><p>[2024/03] Our extension on deepfake detection has been accepted to IJCV.</p></li> -->
<!-- <li><p>[2024/02] Our paper on test-time backdoor defense has been accepted to CVPR 2024.</p></li> -->
<!-- <li><p>[2024/02] Our blogpost on gradient inversion has been accepted to ICLR BlogPosts 2024.</p></li> -->
<!-- <li><p>[2024/01] Two papers have been accepted to ICLR 2024.</p></li> -->
<!-- <li><p>[2023/11] Our paper on heterogeneous federated learning has been accepted to TPAMI.</p></li> -->
<!-- <li><p>[2023/10] Two papers on model selection and uncertainty estimation in domain adaptation have been accepted to NeurIPS DistShift Workshop 2023.</p></li> -->
<!-- <li><p>[2023/09] Our paper on validation of unsupervised domain adaptation methods has been accepted to NeurIPS 2023.</p></li> -->
<!--<li><p>2023/08 Our paper on source-free domain adaptation has been accepted to Neural Networks.</p></li>
<li><p>2023/07 Four papers have been accepted to ICCV 2023.</p></li>
<li><p>2023/07 <a href="https://github.com/yuyongcan/Benchmark-TTA">Benchmark-TTA</a> (a benchmark for test-time adaptation) has been released.</p></li>
<li><p>2023/07 <a href="https://github.com/YuheD/MAPS">MAPS (source-free domain adaptive keypoint detection)</a> has been accepted to IEEE TCSVT (IF: 8.4).</p></li> -->
</ul>

<h1>Recent Work</h1>
<details><summary>
<strong><span id='focus'>Adapting Vision-Language Models Without Labels</span>: A Comprehensive Survey</strong>
<br></summary>
<div id="abstract"><i>
<dd><br>
Abstract: 
</dd><br>
</i></div></details>

Hao Dong<sup>&#9839;</sup>, Lijun Sheng<sup>&#9839;</sup>, Jian Liang<sup>&copy;</sup>, Ran He, Eleni Chatzi, Olga Fink<br>
<i>Arxiv technical report, 2025.</i><br>
<a href="https://arxiv.org/pdf/2508.05547.pdf"><span id="paper">Paper</span></a> / 
<a href="https://arxiv.org/abs/2508.05547"><span id="paperlink">Link</span></a> / 
<a href="hhttps://github.com/tim-learn/Awesome-LabelFree-VLMs"><span id="papercode">Code</span></a>
<br><br>

<details><summary>
<strong>LoRA-Pro: Are <span id='focus'>Low-Rank Adapters</span> Properly Optimized?</strong>
<br></summary>
</details>

Zhengbo Wang, Jian Liang<sup>&copy;</sup>, Ran He, Zilei Wang, Tieniu Tan.<br>
<i>International Conference on Learning Representations (ICLR), Spotlight, 2025.</i><br>
<a href="https://arxiv.org/pdf/2407.18242.pdf"><span id="paper">Paper</span></a> / 
<a href="https://openreview.net/forum?id=gTwRMU3lJ5"><span id="paperlink">Link</span></a> / 
<a href="https://github.com/mrflogs/LoRA-Pro"><span id="papercode">Code</span></a>
<br><br>

<details><summary>
<strong>The Illusion of Progress? A Critical Look at <span id='focus'>Test-Time Adaptation</span> for Vision-Language Models</strong>
<br></summary>
</details>

Lijun Sheng, Jian Liang<sup>&copy;</sup>, Ran He, Zilei Wang, Tieniu Tan.<br>
<i>Arxiv technical report, 2025.</i><br>
<a href="https://arxiv.org/pdf/2506.24000.pdf"><span id="paper">Paper</span></a> / 
<a href="https://arxiv.org/abs/2506.24000"><span id="paperlink">Link</span></a> / 
<a href="https://github.com/TomSheng21/tta-vlm"><span id="papercode">Code</span></a>
<br><br>

<details><summary>
<strong>R-TPT: Improving Adversarial Robustness of Vision-Language Models through <span id='focus'>Test-Time Prompt Tuning</strong>
<br></summary>
</details>

Lijun Sheng, Jian Liang<sup>&copy;</sup>, Zilei Wang, Ran He<br>
<i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025</i><br>
<a href="https://arxiv.org/pdf/2504.11195.pdf"><span id="paper">Paper</span></a> / 
<a href="https://arxiv.org/abs/2504.11195"><span id="paperlink">Link</span></a> / 
<a href="https://github.com/TomSheng21/R-TPT"><span id="papercode">Code</span></a>
<br><br>

<details><summary>
<strong>Test-Time Immunization: A Universal Defense Framework Against <span id='focus'>Jailbreaks</span> for (Multimodal) Large Language Models</strong>
<br></summary>
</details>
Yongcan Yu, Yanbo Wang, Ran He, Jian Liang<sup>&copy;</sup><br>
<i>Arxiv technical report, 2025</i><br>
<a href="https://arxiv.org/pdf/2505.22271.pdf"><span id="paper">Paper</span></a> / 
<a href="https://arxiv.org/abs/2505.22271"><span id="paperlink">Link</span></a>
<br><br>

<details><summary>
<strong>Do We Really Need Curated Malicious Data for <span id='focus'>Safety Alignment</span> in Multi-modal Large Language Models?</strong>
<br></summary>
</details>

Yanbo Wang, Jiyang Guan, Jian Liang<sup>&copy;</sup>, Ran He<br>
<i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025</i><br>
<a href="https://arxiv.org/pdf/2504.10000"><span id="paper">Paper</span></a> / 
<a href="https://arxiv.org/abs/2504.10000"><span id="paperlink">Link</span></a> / 
<a href="https://github.com/ybwang119/MLLM_safety_study"><span id="papercode">Code</span></a>
<br><br>

<details><summary>
<strong>Cooperative Pseudo Labeling for <span id='focus'>Unsupervised Federated Classification</strong>
<br></summary>
</details>

Kuangpu Guo, Lijun Sheng, Yongcan Yu, Jian Liang<sup>&copy;</sup>, Zilei Wang, Ran He<br>
<i>International Conference on Computer Vision (ICCV), 2025.</i><br>
<a href="https://arxiv.org/pdf/2507.xxx.pdf"><span id="paper">Paper</span></a> / 
<a href="https://arxiv.org/abs/2507.xxx.pdf"><span id="paperlink">Link</span></a> / 
<a href="https://github.com/krumpguo/FedCoPL"><span id="papercode">Code</span></a>
<br><br>

<details><summary>
<strong>Exploring Vacant Classes in <span id='focus'>Label-Skewed Federated Learning</strong>
<br></summary>
</details>

Kuangpu Guo, Yuhe Ding, Jian Liang<sup>&copy;</sup>, Ran He, Zilei Wang, Tieniu Tan<br>
<i>AAAI Conference on Artificial Intelligence (AAAI), 2025</i><br>
<a href="https://arxiv.org/pdf/2401.02329.pdf"><span id="paper">Paper</span></a> / 
<a href="https://arxiv.org/abs/2401.02329"><span id="paperlink">Link</span></a> / 
<a href="https://github.com/krumpguo/FedVLS"><span id="papercode">Code</span></a>
<br><br>

<details><summary>
<strong><span id='focus'>Ranking Vision-Language Models</span> in Fully Unlabeled Tasks</strong>
<br></summary>
<div id="abstract"><i>
<dd><br>
Abstract: 
</dd><br>
</i></div></details>

Yuhe Ding, Bo Jiang, Aihua Zheng, Qin Xu, Jian Liang<br>
<i>IEEE Transactions on Multimedia (TMM), 2025</i><br>
<a href="https://arxiv.org/pdf/2412.20682.pdf"><span id="paper">Paper</span></a> / 
<a href="https://arxiv.org/abs/2412.20682"><span id="paperlink">Link</span></a>
<br><br>

<details><summary>
<strong><span id='focus'>Out-of-Distribution Detection</span>: A Task-Oriented Survey of Recent Advances</strong>
<br></summary>
</details>

Shuo Lu, Yingsheng Wang, Lijun Sheng, Aihua Zheng, Lingxiao He, Jian Liang<sup>&copy;</sup><br>
<i>ACM Computing Surveys (CSUR), 2025</i><br>
<a href="https://arxiv.org/pdf/2409.11884.pdf"><span id="paper">Paper</span></a> / 
<a href="https://dl.acm.org/doi/10.1145/3760390"><span id="paperlink">Link</span></a> / 
<a href="https://github.com/shuolucs/Awesome-Out-Of-Distribution-Detection"><span id="papercode">Code</span></a>
<br><br>

<details><summary>
<strong>Uni-Layout: Integrating Human Feedback in Unified <span id='focus'>Layout Generation</span> and Evaluation</strong>
<br></summary>
</details>

Shuo Lu, Yanyin Chen, Wei Feng, Jiahao Fan, Fengheng Li, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law, Jian Liang<sup>&copy;</sup><br>
<i>ACM International Conference on Multimedia (ACM MM), 2025</i><br>
<a href="https://arxiv.org/pdf/2508.02374.pdf"><span id="paper">Paper</span></a> / 
<a href="https://arxiv.org/abs/2508.02374"><span id="paperlink">Link</span></a> / 
<a href="https://github.com/JD-GenX/Uni-Layout"><span id="papercode">Code</span></a>
<br><br>


<br>
<div id="footer">
<div id="footer-text">
Last updated on July, 2025. The CSS file is adapted from <a href="https://people.eecs.berkeley.edu/~brecht/" target="blank">Ben Recht's webpage</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>